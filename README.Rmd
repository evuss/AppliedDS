---
title: "Applied Data Science course material"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
Packages we'll look at today:

- odbc / readxl / readr / dbplyr for data access 
- tidyverse for data manipulation
- DataExplorer for providing an overview of our data
- modelr / rsamples for sampling strategy
- recipes for performing feature engineering
- glmnet / h2o / FFTrees for building models
- yardstick / broom for evaluation
- rmarkdown for documentation


## Working with databases
We need a database connection before we can do anything with our database. Add it by the below code or go o Connections > New Connection. Inside the "New Connection" we can get the connection code. 

```{r}
library(DBI)
library(odbc)

Driver = "SQL Server"
Server = "fbmcsads.database.windows.net"
Database = "WideWorldImporters-Standard"
Uid = "adatumadmin"
Pwd = "Pa55w.rdPa55w.rd"


con <- dbConnect(odbc(),
                 driver=Driver,
                 server=Server,
                 database=Database,
                 pwd=Pwd,
                 uid=Uid
                 )
```

Now that we have a DB connection, we can write SQL in a code chunk. We use the above connection name "con". 

```{sql connection=con} 
select top 5 * from flights

```

We can use bdplyr to construct dplyr commands that work on the DB. 

```{r}
library(tidyverse)
library(dbplyr) # tip: read the introduction to dbplyr package to know how to connect to db
flights_tbl <- tbl(con, "flights")

flights_tbl %>% 
  filter(month<=6) %>% 
  group_by(origin) %>% 
  summarise(n = n(), 
            mean_dist= mean(distance)) %>% 
  show_query()
```
We can also work with tables that aren'nt in the default schema.

```{r}
purchaseorders_tbl <- tbl(con, in_schema("purchasing", "purchaseorders")) # tip: see the documentation for in_schema in the dbplyr package to know hos to refer to a table in a schema. 

purchaseorders_tbl %>% 
  top_n(5)

```

To write SQL updates, create views etc - it is better to use the DBI packages instead of dbplyr. 

We can use the 'Id()' function from DBI to work with schema more generically within a database. This means we aren't restricted to just SELECT statements. 

```{r error=TRUE}
# Create a schema to work in - errors if already exists
dbGetQuery(con, "CREATE SCHEMA DBIexampleEVA") # creates my own schema in db
# Write some data / drop & recreate the table if it exists already
dbWriteTable(con, "iris", iris, overwrite=TRUE) # overwrites the table
# Read from newly written table 
head(dbReadTable(con,"iris"))
# Read from a table in a schema
head(dbReadTable(con,Id(schema="20774A", table="CustomerTransactions")))
# If a write method is supported by the driver, this will work
dbWriteTable(con, Id( schema="DBIexampleEVA", table="iris", overwrite=TRUE))

```

Some of our code could fail in that section so we used `error=TRUE` to be able to carry on even if some of the code errored. Great for optional cose or things with bad connection. 

## Explorative Data Analysis (EDA) using the DataExplorer package
High-level report to help us understand the data. 
Group discrete features into a "other" category. 
Make sure only to include the scheduled hour and not the 

```{r eval=FALSE}
flights_tbl %>% 
  as_data_frame() %>% 
  DataExplorer::GenerateReport()

```
### Questions
Questions arising from the basic report:

1. Why is there a day with double the number of flights?
2. The date 31:st doesn't have as many flights as other days, do we need to adjust for this?
3. Do we need to do anything about missings or can we just remove the rows?
4. Why is there negative correlation between `flights` (flight number) and `distance`?

Things to implement later in the workflow due to the EDA:
1. We need to address the high correlation between time columns
2. We need to group low frequency airline carriers
3. Bivariate analysis

### Answering our questions

> Why is there a day with double the number of flights?

Are there duplicate rows?

```{r, eval=FALSE}
flights_tbl %>% 
  filter(day==15) %>% 
  distinct() %>% 
  summarise(n()) %>% 
  as_data_frame()->
  distinct_count

flights_tbl %>% 
  filter(day==15) %>% 
  summarise(n())%>% 
  as_data_frame() ->
  row_count

identical(row_count,distinct_count)
```

If the identical() is TRUE, they are exactly equal, meaning that we not have any duplicate rows. But are the number of rows unusual?

```{r}
library(ggplot2)
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n=n(), n_distinct(flight)) %>% 
  as_data_frame() %>% 
  ggplot(aes(x=day, y=n)) + geom_col()
 
```
Looks like the jump in the histogram is an artifact with the data visualization binning data. 

### Bivariate analysis
Next answer

> Bivariate analysis

```{r}

flights_tbl %>% 
  select_if(is.numeric) %>% 
  as_data_frame() %>% 
  gather(col, val, -dep_delay) %>% # takes our wide data and turn it into long data, aka pivot all columns without the dep_delay.
  filter(col!="arr_delay",
         dep_delay<500) %>% 
  ggplot(aes(x=val, y=dep_delay)) +
  #  geom_point() +  # this will take long time since it is plotting row by row
  geom_bin2d() + 
   facet_wrap(~col, scales = "free") # take different parts of our data to produce them as charts

```
## Sampling: Inbalanced data

### Theory/info

In our data model, 97% are delayed. This means that the model always will predict "delayed". We have some options to deal with this. 

- Undersampling (downsampling). Consequence: reducing data since we reduce the data to fit out imbalanced data. 
- Upsampling. Consequence: risks of overfitting. Doesn't reduce training data
- Synthesising data makes extra records that are like the minority class (Doesn't reduce training set, Avoids some of the overfit risk of upsampling, Can weaken predictions if minority data is very similair to majority)

We need to think about whether we need to k-fold cross-validation explicitly.

- Run the same model and assess robustness of coefficients
- We have an algorithm that needs explicit cross validation because it doesn't do it internally
- When we're going to run lots of models with hyper-parameter tuning do the results are more consistent

We use bootstrapping when we want t fit a single model and ensure the results are robust. This will often do many more iterations than k-fold cross validation, making it better in cases where there's relatively small amounts of data. 

Packages we can use for sampling incude:

- modelr which facilitates bootstrap and k-fold cross validation strategies
- rsample allows us to bootstrap and perform a wide variety of cross validation tasks
- recipes allows us to upsample and downsample
- synthpop allows us to build synthesised samples



```{r}
## The devtools are not updated in our cran verison, so we install them separate 
install.packages("devtools")
devtools::install_github("topepo/recipes")
```

### Practical
```{r}
flights_tbl %>% 
  as_data_frame() ->
  flights #insert our flights data to in-memory to be able to use the chosen packages

flights %>% 
  mutate(was_delayed=arr_delay>5) -> # here we adding columns that are not related to predicting variables, like changing time to hours 
  flights

flights %>% 
  modelr::resample_partition(c(train=0.7, test=0.3)) ->
  splits

splits %>% 
  pluck("train") %>% 
  as_data_frame()->
  train_raw

splits %>% 
  pluck("test") %>% 
  as_data_frame()->
  test_raw
  
```

During the investigation we will look at the impact of upsampling. We'll see it in action in a bit. First prepping our basic features!

#### Basic Feature Engineering

```{r}
library(recipes)

basic_fe <- recipe(train_raw, was_delayed~. ) # we use was_delayed as predictor and everything else as variables

basic_fe %>% 
  step_rm(ends_with("time"), ends_with("delay"), tailnum, flight, minute, time_hour) %>%  # remove some variables
 # step_corr(all_predictors()) %>% # remove highly correlates with other variables
  step_zv(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_naomit(all_predictors()) %>% # remove rows that have missing values, since only 3 perc are missing
  step_other(all_nominal(), threshold = 0.03)-> # will pool infrequently occuring values into an "other category" 
  colscleaned_fe

colscleaned_fe
colscleaned_fe <- prep(colscleaned_fe, verbose = TRUE) # if error, type verbose=TRUE to print more error info

colscleaned_fe

train_prep1<-bake(colscleaned_fe, train_raw) # use the recipe to clean the train dataset

```
Now we need to process our numeric variables.

```{r}
colscleaned_fe %>% 
  step_log(distance) %>% 
  step_num2factor(month, day, hour) ->
  numscleaned_fe

numscleaned_fe
numscleaned_fe <- prep(numscleaned_fe, verbose = TRUE)
numscleaned_fe

train_prep1<-bake(numscleaned_fe, train_raw)
```


